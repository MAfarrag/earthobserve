{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498f4321-5703-49ba-9299-b5b049be992c",
   "metadata": {},
   "source": [
    "# Download the ERA5 data from Amazon S3 data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e5c27-cc83-4a23-b091-79b22d038dc2",
   "metadata": {},
   "source": [
    "In this script we will do the following\n",
    "- Download ERA5 precipitation data from Amazon S3 using earth2observe.\n",
    "- Change the format of the downloaded data from netcdf to rasters each represent one time stamp.\n",
    "- Create Index map, to refer to the location of each cell with an index number.\n",
    "- Create point (at the center of the cell) and polygon (covers the whole cell) geometries to use also as an index.\n",
    "- Convert the rasters into columns in a dataframe.\n",
    "- Use Uber H3 spatial index to get the spatial index for all cells for all 16 resolution.\n",
    "\n",
    "WE will be using\n",
    "- earth2observe package\n",
    "- The convert module in the pyramids package (dependency of earth2observe you don't need to install it )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eccf30-2b0d-4fe0-8d6b-cd5b37c358a5",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88461869-2de6-4733-85e2-9db653d868cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "from loguru import logger\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyramids.raster import Raster\n",
    "from pyramids.convert import Convert\n",
    "from osgeo import gdal\n",
    "from osgeo.gdal import Dataset\n",
    "import numpy as np\n",
    "from pyramids.indexing import H3\n",
    "from earth2observe.earth2observe import Earth2Observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4197e29-4b73-4bb7-bfce-88b605f30144",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e4941-2dd7-4d10-bb6f-49938795bbfb",
   "metadata": {},
   "source": [
    "- First define the root directory where all the data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7e0c52-8c34-4b72-8fc0-b5a1b43427ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdir = \"project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d45016c-2b6e-4a92-8465-9ab49505b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\gdrive\\01Algorithms\\Hydrology\\earth2observe\\examples\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2132a70-f4de-43bc-8701-85f10bb94ab6",
   "metadata": {},
   "source": [
    "The directory should have 4 folders\n",
    "project\\\n",
    "    files\\\n",
    "    metadata\\\n",
    "    rasters\\\n",
    "    s3-backend\\\n",
    "    \n",
    "files: \n",
    "    Contains the processed data saved in a parquet format\n",
    "metadata: contains 3 files \n",
    "    - index.tif: index raster.\n",
    "    - index_points.parquet: index point geometry.\n",
    "    - index_polygon.parquet: index polygon geometry.\n",
    "rasters: \n",
    "    Cotains the 1-band raster files converted from the downloaded netcdf, each raster represent 1 time step in the downloaded data time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9b386-7152-420f-9297-582990dd2f85",
   "metadata": {},
   "source": [
    "## Earth2observe abstract class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d82a8100-f04f-4db2-96e7-f46a3a9367e0",
   "metadata": {},
   "source": [
    "- Second define the earth2observe parameters.\n",
    "\n",
    "data_source: [str]\n",
    "    data source name. the available data sources are \n",
    "    [\"ecmwf\", \"chirps\", \"amazon-s3\"].\n",
    "temporal_resolution (str, optional):\n",
    "    temporal resolution. Defaults to 'daily'.   \n",
    "start (str, optional):\n",
    "    start date. Defaults to ''.\n",
    "end (str, optional):\n",
    "    end date. Defaults to ''.\n",
    "path (str, optional):\n",
    "    Path where you want to save the downloaded data. \n",
    "    Defaults to ''.\n",
    "variables (list, optional):\n",
    "    Variable name.\n",
    "lat_lim (list, optional):\n",
    "    [ymin, ymax]. Defaults to None.\n",
    "lon_lim (list, optional):\n",
    "    [xmin, xmax]. Defaults to None.\n",
    "fmt (str, optional):\n",
    "    date format. Defaults to \"%Y-%m-%d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29746497-9401-4b52-b5a7-f459aad9143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |--------------------------------------------------| 0.0% CompleteDownloading 2022/05/data/precipitation_amount_1hour_Accumulation.nc from S3...\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "start = \"2022-05-01\"\n",
    "end = \"2022-05-01\"\n",
    "time = \"monthly\"\n",
    "path = f\"{rdir}/s3-backend\"\n",
    "source = \"amazon-s3\"\n",
    "variables = [\"precipitation\"]\n",
    "e2o = Earth2Observe(\n",
    "    data_source=source,\n",
    "    temporal_resolution=time,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    path=path,\n",
    "    variables=variables,\n",
    ")\n",
    "\n",
    "e2o.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48844b1c-f39b-4fcb-9f08-6a4052fcc928",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11bc2c-3204-49cf-a507-b41eb00237da",
   "metadata": {},
   "source": [
    "- Convert the downloaded netcdf into rasters one for each time stamp in the ncdf file For the example I converted only 1-hourly rasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bd178c-4d5a-4b1a-9b9b-e488adbf0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = f\"{path}/202205_monthly_precipitation_amount_1hour_Accumulation.nc\"\n",
    "save_to = f\"{rdir}/rasters\"\n",
    "Convert.nctoTiff(nc_file, save_to, time_var_name=\"time1\", prefix=\"Amazon-S3-ERA5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e75fe8-2678-4c97-997d-1b6693992ad8",
   "metadata": {},
   "source": [
    "In this part we will create a spatial index for each cell in the downloaded rasters, and convert the rasters into a \n",
    "pandas dataframe, \n",
    "- First spatial indexing method, we will create an index raster with an id for each cell that will refer to the row in \n",
    "the dataframe to be able to locate the value and associate it to a specific location.\n",
    "- Second method we will create a point/polygon geometry at the center of each cell so we can query the whole raster but \n",
    "using geometries relations\n",
    "- Third we will use the H3 indexing method so we can assign a hexadecimal index (for each resolution 0-15) so we can \n",
    "use the different resolution of H3 tfor faster querying of data. \n",
    "- The creation of the polygon index will take a bit long time (3 min) but it is optional since we can only use the \n",
    "point index\n",
    "- So the point/polygon and raster index will be created only once since all rasters have the same dimensions\n",
    "- After converting all rasters into a dataframe ewe will use the point index to get the H3 index for all points for \n",
    "the 16 resolutions and add them to the same dataframe.\n",
    "- In the last step we will save the dataframe as a parquet data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cff65-a40e-4137-a11c-832dbadb9332",
   "metadata": {},
   "source": [
    "- In the following function we defined all the above steps and we will call the function and use one of the rasters in the rasters folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae6cfcf-d06a-48a6-ac1e-eeeed81c10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo.gdal import Dataset\n",
    "\n",
    "def create_metadata(src: Dataset, path: str):\n",
    "    \"\"\"Create the index raster and the geometry file (both point and polygon)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src: [Dataset]\n",
    "        gdal Dataset.\n",
    "    path: [str]\n",
    "        path to where the metadata are going to be saved.\n",
    "    \"\"\"\n",
    "    # first create the raster\n",
    "    logger.info(\"First step (creating index raster)\")\n",
    "    arr = src.ReadAsArray()\n",
    "    rows, cols = arr.shape\n",
    "\n",
    "    unique_nums = list(range(1, rows * cols + 1))\n",
    "    arr = np.array(unique_nums)\n",
    "    new_arr = np.reshape(arr, (rows, cols))\n",
    "    dst= Raster.rasterLike(src, new_arr, driver=\"MEM\")\n",
    "    Raster.saveRaster(dst, f\"{path}/index.tif\")\n",
    "    # second create the point index file from the index raster\n",
    "    logger.info(\"Second step (Create index point geometry file)\")\n",
    "    logger.info(\"The Point geometry will be created at the center of each cell so we can query the cells values by \"\n",
    "                \"indexing the cell center location\")\n",
    "    logger.info(\"This step might take couple of minutes but these step are executed only once to create the metadata\")\n",
    "    gdf = Convert.rasterToGeoDataFrame(dst, add_geometry=\"point\")\n",
    "    gdf.to_parquet(f\"{path}/index_points.parquet\", index=False, compression='gzip')\n",
    "    # third create the polygon index file from the index raster\n",
    "    logger.info(\"Third step (Create index polygon geometry file)\")\n",
    "    gdf = Convert.rasterToGeoDataFrame(dst, add_geometry=\"polygon\")\n",
    "    gdf.to_parquet(f\"{path}/index_polygon.parquet\", index=False, compression='gzip')\n",
    "    logger.info(\"Creating index data has finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ac037-9ed9-40fa-bb39-7527817d19d3",
   "metadata": {},
   "source": [
    "- using `glob` we will get all the rasters in the rasters folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f012da92-5416-45c7-a343-de73abd7ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project/rasters\\\\Amazon-S3-ERA5_2022.05.01.00.00.00.tif', 'project/rasters\\\\Amazon-S3-ERA5_2022.05.01.01.00.00.tif', 'project/rasters\\\\Amazon-S3-ERA5_2022.05.01.02.00.00.tif', 'project/rasters\\\\Amazon-S3-ERA5_2022.05.01.03.00.00.tif']\n"
     ]
    }
   ],
   "source": [
    "search_criteria = \"*.tif\"\n",
    "file_list = glob.glob(os.path.join(f\"{rdir}/rasters\", search_criteria))\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a99489-6101-480a-9405-fada31842739",
   "metadata": {},
   "source": [
    "- Now we will call the `create_metadata` function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60885ef6-2088-4120-8feb-ffed08eafee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 20:19:20.601 | INFO     | __main__:create_metadata:14 - First step (creating index raster)\n",
      "2023-01-29 20:19:20.681 | INFO     | __main__:create_metadata:24 - Second step (Create index point geometry file)\n",
      "2023-01-29 20:19:20.681 | INFO     | __main__:create_metadata:25 - The Point geometry will be created at the center of each cell so we can query the cells values by indexing the cell center location\n",
      "2023-01-29 20:19:20.682 | INFO     | __main__:create_metadata:27 - This step might take couple of minutes but these step are executed only once to create the metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 20:20:50.201 | INFO     | __main__:create_metadata:31 - Third step (Create index polygon geometry file)\n",
      "2023-01-29 20:22:36.460 | INFO     | __main__:create_metadata:34 - Creating index data has finished successfully\n"
     ]
    }
   ],
   "source": [
    "fname = file_list[0]\n",
    "src = gdal.Open(fname)\n",
    "meta_data_path = f\"{rdir}/metadata\"\n",
    "create_metadata(src, meta_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456165ba-3038-421c-b4d4-62c77b6fd7f9",
   "metadata": {},
   "source": [
    "## Convert the downloaded data into dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb22a0-ce08-4c81-ba68-539fa6ed7422",
   "metadata": {},
   "source": [
    "In this part we will convert the rasters into Dataframe using the convert module in the Pyramids package [https://pyramids-gis.readthedocs.io/en/latest](https://pyramids.readthedocs.io/en/latest).\n",
    "- The Pyramids package is a GIS utility package that handles raster and vector data in addition to multiple other dataformat.\n",
    "- In the convert module in the pyramids package there are couple of function that can convert data from format to another like rasterToPolygon, polygonToRaster, and rasterToGeoDataFrame\n",
    "- for more information on how the rasteToGeodataFrame function works you can check the [rasterToGeoDataFrame documentation](https://pyramids-gis.readthedocs.io/en/latest/convert.html#rastertogeodataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148aa4b1-e68c-4480-9b36-217552666614",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = src.RasterYSize\n",
    "cols = src.RasterXSize\n",
    "fmt = \"%Y.%m.%d.%H.%M.%S\"\n",
    "hourly_fmt = \"%Y-%m-%d-%H\"\n",
    "data = np.zeros(shape=(rows * cols, len(file_list))) * np.nan\n",
    "file_order = []\n",
    "for i, fname in enumerate(file_list):\n",
    "    date_fragments = fname.split(\"_\")[-1][:-4]\n",
    "    file_order.append(dt.datetime.strptime(date_fragments, fmt))\n",
    "    data[:, i] = Convert.rasterToGeoDataFrame(fname).values.reshape((rows*cols))\n",
    "\n",
    "col_names = [date_i.strftime(hourly_fmt) for date_i in file_order]\n",
    "# making the date as an index makes the files size grows drastically\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "df.to_parquet(f\"{rdir}/files/data.parquet\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082faa9-4959-4e75-a1f9-9d299dac14fe",
   "metadata": {},
   "source": [
    "- Now we can check the `df` to see what is stored there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80068a97-88e1-4af2-8792-75b81421fb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022-05-01-00</th>\n",
       "      <th>2022-05-01-01</th>\n",
       "      <th>2022-05-01-02</th>\n",
       "      <th>2022-05-01-03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2022-05-01-00  2022-05-01-01  2022-05-01-02  2022-05-01-03\n",
       "0       0.000061            0.0            0.0       0.000061\n",
       "1       0.000061            0.0            0.0       0.000061\n",
       "2       0.000061            0.0            0.0       0.000061\n",
       "3       0.000061            0.0            0.0       0.000061\n",
       "4       0.000061            0.0            0.0       0.000061"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127de83-75b0-4d23-837c-801c02a87530",
   "metadata": {},
   "source": [
    "## Indexing the data with h3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a7961-74b9-4c2b-aa8d-d55c49e4125d",
   "metadata": {},
   "source": [
    "- Read the parquet file containing the extracted cell values and generating the H3 index for each resolution level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86399b4-e7ea-4110-9ea6-b925ff9ad247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the coordinates from each point in the point index geometry file we created in the last step to use it in obtaining the h3 index for different resolutions\n",
      "H3 resolution :0\n",
      "H3 resolution :1\n",
      "H3 resolution :2\n",
      "H3 resolution :3\n",
      "H3 resolution :4\n",
      "H3 resolution :5\n",
      "H3 resolution :6\n",
      "H3 resolution :7\n",
      "H3 resolution :8\n",
      "H3 resolution :9\n",
      "H3 resolution :10\n",
      "H3 resolution :11\n",
      "H3 resolution :12\n",
      "H3 resolution :13\n",
      "H3 resolution :14\n",
      "H3 resolution :15\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(f\"{rdir}/files/data.parquet\")\n",
    "# read the point index file and index\n",
    "point_index = gpd.read_parquet(f\"{rdir}/metadata/index_points.parquet\")\n",
    "print(\"Extract the coordinates from each point in the point index geometry file we created in the last step to use it in obtaining the h3 index for different resolutions\")\n",
    "coords = [(i.x, i.y) for i in point_index[\"geometry\"]]\n",
    "\n",
    "for res in range(16):\n",
    "    print(f\"H3 resolution :{res}\")\n",
    "    hex = [H3.geometryToIndex(xy[1], xy[0], res) for xy in coords]\n",
    "    # hex = H3.getIndex(point_index, res)\n",
    "    df[f\"{res}\"] = hex\n",
    "\n",
    "df.to_parquet(f\"{rdir}/files/data.parquet\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee5331-149c-4f9f-95af-80a1823778cc",
   "metadata": {},
   "source": [
    "Now all the preprocessing tasks is done and you have the data saved in the parquet data format, we can read it and \n",
    "query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1251fc02-f518-4014-950b-0f909b25de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"{rdir}/files/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efcc5afd-67cc-4a06-ad8e-66ad9ac97ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022-05-01-00</th>\n",
       "      <th>2022-05-01-01</th>\n",
       "      <th>2022-05-01-02</th>\n",
       "      <th>2022-05-01-03</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>80f3fffffffffff</td>\n",
       "      <td>81f2bffffffffff</td>\n",
       "      <td>82f297fffffffff</td>\n",
       "      <td>83f293fffffffff</td>\n",
       "      <td>84f2939ffffffff</td>\n",
       "      <td>85f29397fffffff</td>\n",
       "      <td>86f293957ffffff</td>\n",
       "      <td>87f293956ffffff</td>\n",
       "      <td>88f293956bfffff</td>\n",
       "      <td>89f293956afffff</td>\n",
       "      <td>8af293956ac7fff</td>\n",
       "      <td>8bf293956ac2fff</td>\n",
       "      <td>8cf293956ac23ff</td>\n",
       "      <td>8df293956ac223f</td>\n",
       "      <td>8ef293956ac2237</td>\n",
       "      <td>8ff293956ac2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>80f3fffffffffff</td>\n",
       "      <td>81f2bffffffffff</td>\n",
       "      <td>82f297fffffffff</td>\n",
       "      <td>83f293fffffffff</td>\n",
       "      <td>84f2939ffffffff</td>\n",
       "      <td>85f29397fffffff</td>\n",
       "      <td>86f293957ffffff</td>\n",
       "      <td>87f293956ffffff</td>\n",
       "      <td>88f293956bfffff</td>\n",
       "      <td>89f293956afffff</td>\n",
       "      <td>8af293956ac7fff</td>\n",
       "      <td>8bf293956ac3fff</td>\n",
       "      <td>8cf293956ac33ff</td>\n",
       "      <td>8df293956ac337f</td>\n",
       "      <td>8ef293956ac3347</td>\n",
       "      <td>8ff293956ac3341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>80f3fffffffffff</td>\n",
       "      <td>81f2bffffffffff</td>\n",
       "      <td>82f297fffffffff</td>\n",
       "      <td>83f293fffffffff</td>\n",
       "      <td>84f2939ffffffff</td>\n",
       "      <td>85f29397fffffff</td>\n",
       "      <td>86f293957ffffff</td>\n",
       "      <td>87f293956ffffff</td>\n",
       "      <td>88f293956bfffff</td>\n",
       "      <td>89f293956afffff</td>\n",
       "      <td>8af293956acffff</td>\n",
       "      <td>8bf293956ac8fff</td>\n",
       "      <td>8cf293956ac8dff</td>\n",
       "      <td>8df293956ac8c3f</td>\n",
       "      <td>8ef293956ac8c17</td>\n",
       "      <td>8ff293956ac8c15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>80f3fffffffffff</td>\n",
       "      <td>81f2bffffffffff</td>\n",
       "      <td>82f297fffffffff</td>\n",
       "      <td>83f293fffffffff</td>\n",
       "      <td>84f2939ffffffff</td>\n",
       "      <td>85f29397fffffff</td>\n",
       "      <td>86f293957ffffff</td>\n",
       "      <td>87f293956ffffff</td>\n",
       "      <td>88f293956bfffff</td>\n",
       "      <td>89f293956afffff</td>\n",
       "      <td>8af293956acffff</td>\n",
       "      <td>8bf293956ac9fff</td>\n",
       "      <td>8cf293956ac9dff</td>\n",
       "      <td>8df293956ac9d7f</td>\n",
       "      <td>8ef293956ac9c67</td>\n",
       "      <td>8ff293956ac9c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>80f3fffffffffff</td>\n",
       "      <td>81f2bffffffffff</td>\n",
       "      <td>82f297fffffffff</td>\n",
       "      <td>83f293fffffffff</td>\n",
       "      <td>84f2939ffffffff</td>\n",
       "      <td>85f29397fffffff</td>\n",
       "      <td>86f293957ffffff</td>\n",
       "      <td>87f293956ffffff</td>\n",
       "      <td>88f293950dfffff</td>\n",
       "      <td>89f293950dbffff</td>\n",
       "      <td>8af293950d97fff</td>\n",
       "      <td>8bf293950d94fff</td>\n",
       "      <td>8cf293950d949ff</td>\n",
       "      <td>8df293950d948bf</td>\n",
       "      <td>8ef293950d948af</td>\n",
       "      <td>8ff293950d948a9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2022-05-01-00  2022-05-01-01  2022-05-01-02  2022-05-01-03  \\\n",
       "0       0.000061            0.0            0.0       0.000061   \n",
       "1       0.000061            0.0            0.0       0.000061   \n",
       "2       0.000061            0.0            0.0       0.000061   \n",
       "3       0.000061            0.0            0.0       0.000061   \n",
       "4       0.000061            0.0            0.0       0.000061   \n",
       "\n",
       "                 0                1                2                3  \\\n",
       "0  80f3fffffffffff  81f2bffffffffff  82f297fffffffff  83f293fffffffff   \n",
       "1  80f3fffffffffff  81f2bffffffffff  82f297fffffffff  83f293fffffffff   \n",
       "2  80f3fffffffffff  81f2bffffffffff  82f297fffffffff  83f293fffffffff   \n",
       "3  80f3fffffffffff  81f2bffffffffff  82f297fffffffff  83f293fffffffff   \n",
       "4  80f3fffffffffff  81f2bffffffffff  82f297fffffffff  83f293fffffffff   \n",
       "\n",
       "                 4                5                6                7  \\\n",
       "0  84f2939ffffffff  85f29397fffffff  86f293957ffffff  87f293956ffffff   \n",
       "1  84f2939ffffffff  85f29397fffffff  86f293957ffffff  87f293956ffffff   \n",
       "2  84f2939ffffffff  85f29397fffffff  86f293957ffffff  87f293956ffffff   \n",
       "3  84f2939ffffffff  85f29397fffffff  86f293957ffffff  87f293956ffffff   \n",
       "4  84f2939ffffffff  85f29397fffffff  86f293957ffffff  87f293956ffffff   \n",
       "\n",
       "                 8                9               10               11  \\\n",
       "0  88f293956bfffff  89f293956afffff  8af293956ac7fff  8bf293956ac2fff   \n",
       "1  88f293956bfffff  89f293956afffff  8af293956ac7fff  8bf293956ac3fff   \n",
       "2  88f293956bfffff  89f293956afffff  8af293956acffff  8bf293956ac8fff   \n",
       "3  88f293956bfffff  89f293956afffff  8af293956acffff  8bf293956ac9fff   \n",
       "4  88f293950dfffff  89f293950dbffff  8af293950d97fff  8bf293950d94fff   \n",
       "\n",
       "                12               13               14               15  \n",
       "0  8cf293956ac23ff  8df293956ac223f  8ef293956ac2237  8ff293956ac2234  \n",
       "1  8cf293956ac33ff  8df293956ac337f  8ef293956ac3347  8ff293956ac3341  \n",
       "2  8cf293956ac8dff  8df293956ac8c3f  8ef293956ac8c17  8ff293956ac8c15  \n",
       "3  8cf293956ac9dff  8df293956ac9d7f  8ef293956ac9c67  8ff293956ac9c64  \n",
       "4  8cf293950d949ff  8df293950d948bf  8ef293950d948af  8ff293950d948a9  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed881d-4fb1-47fd-962f-b7a0a8700583",
   "metadata": {},
   "source": [
    "- So the now the column names are of type datetime object so you can query it using two dates to get all time steps in between."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
